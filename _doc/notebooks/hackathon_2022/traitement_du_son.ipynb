{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55215814",
   "metadata": {},
   "source": [
    "# Son\n",
    "\n",
    "Quelques éléments de code pour le hackathon 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdef0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jyquickhelper import add_notebook_menu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da8d589",
   "metadata": {},
   "source": [
    "## Télécharger depuis youtube\n",
    "\n",
    "[pytube](https://pytube.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e44089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "# YouTube('https://youtu.be/9bZkp7q19f0').streams.first().download()\n",
    "yt = YouTube('https://www.youtube.com/watch?v=X-4UPGVxKgc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1d7437",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#yt.streams.first().download()\n",
    "down = yt.streams.first().download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d1aa06",
   "metadata": {},
   "source": [
    "Si cela ne fonctionne pas, voir [pytube.exceptions.RegexMatchError: get_throttling_function_name: could not find match for multiple](https://stackoverflow.com/questions/68945080/pytube-exceptions-regexmatcherror-get-throttling-function-name-could-not-find/71903013#71903013)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61003932",
   "metadata": {},
   "outputs": [],
   "source": [
    "down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319fdc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor as me\n",
    "dat = me.AudioFileClip(down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c8a572",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav = dat.to_soundarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0453e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f567b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.write_audiofile(\"sound.wav\", 44100, 2, 2000, \"pcm_s32le\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e58e5c",
   "metadata": {},
   "source": [
    "## pyannote.autio\n",
    "\n",
    "Il faut utiliser la version de github.\n",
    "``pip install git+https://github.com/pyannote/pyannote-audio.git@develop#egg=pyannote-audio``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1cb9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\")\n",
    "\n",
    "# apply pretrained pipeline\n",
    "diarization = pipeline(\"sound.wav\")\n",
    "\n",
    "# print the result\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771c078f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39d13b84",
   "metadata": {},
   "source": [
    "## ONNX\n",
    "\n",
    "Voir [Speaker Diarization using GRU in PyTorch](https://github.com/WiraDKP/pytorch_gru_speaker_diarization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94cfce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "sess = ort.InferenceSession(\"speaker_diarization.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4c40a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sess.get_inputs():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24d6c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "\n",
    "def zcr_vad(y, shift=0.025, win_len=2048, hop_len=1024, threshold=0.005):\n",
    "    if isinstance(y, torch.Tensor):\n",
    "        y = y.cpu().numpy()\n",
    "    if y.ndim == 2:\n",
    "        y = y[0]\n",
    "    zcr = librosa.feature.zero_crossing_rate(y + shift, win_len, hop_len)[0]\n",
    "    activity = gaussian_filter1d(zcr, 1) > threshold\n",
    "    activity = np.repeat(activity, len(y) // len(activity) + 1)\n",
    "    activity = activity[:len(y)]\n",
    "    return activity\n",
    "\n",
    "\n",
    "def get_timestamp(activity):\n",
    "    mask = [k for k, _ in groupby(activity)]\n",
    "    change = np.argwhere(activity[:-1] != activity[1:]).flatten()\n",
    "    span = np.concatenate([[0], change, [len(activity)]])\n",
    "    span = list(zip(span[:-1], span[1:]))\n",
    "    span = np.array(span)[mask]\n",
    "    return span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa61273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from torchaudio.transforms import MFCC, Resample\n",
    "\n",
    "sr = 16000\n",
    "print(\"A\")\n",
    "waveform, ori_sr = torchaudio.load(\"sound.wav\")\n",
    "print(\"A\")\n",
    "waveform = waveform.mean(0, keepdims=True)\n",
    "print(\"A\")\n",
    "_resample = Resample(ori_sr, sr)\n",
    "print(\"A\")\n",
    "audio = _resample(waveform)\n",
    "print(\"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67543a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity = zcr_vad(y)\n",
    "spans = get_timestamp(activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bd1f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = [self._encode_segment(y, span) for span in spans]\n",
    "embed = torch.cat(embed).cpu().numpy()\n",
    "speakers = OptimizedAgglomerativeClustering().fit_predict(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64fb32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3b8ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MFCC, Resample\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class BaseLoad:\n",
    "    def __init__(self, sr=16000, n_mfcc=40):\n",
    "        self.sr = sr\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self._mfcc = MFCC(sr, n_mfcc=40, log_mels=True)\n",
    "        \n",
    "    def _load(self, path, mfcc=True):\n",
    "        try:\n",
    "            waveform, ori_sr = torchaudio.load(path)\n",
    "            waveform = waveform.mean(0, keepdims=True)\n",
    "        except RuntimeError:\n",
    "            raise Exception(f\"Error loading {path}\")\n",
    "        _resample = Resample(ori_sr, self.sr)\n",
    "        audio = _resample(waveform)\n",
    "\n",
    "        if mfcc:\n",
    "            audio = self._mfcc(audio)\n",
    "        return audio\n",
    "\n",
    "\n",
    "class BasePredictor(BaseLoad):\n",
    "    def __init__(self, config_path, max_frame, hop):\n",
    "        config = torch.load(config_path)\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        super().__init__(config.get('sr', 16000), config.get('n_mfcc', 40))\n",
    "        self.ndim = config.get('ndim', 3)\n",
    "        self.max_frame = max_frame\n",
    "        self.hop = hop\n",
    "        \n",
    "    @staticmethod\n",
    "    def _plot_diarization(y, spans, speakers):\n",
    "        c = y[0].cpu().numpy().copy()\n",
    "        for (start, end), speaker in zip(spans, speakers):\n",
    "            c[start:end] = speaker\n",
    "\n",
    "        plt.figure(figsize=(15, 2))\n",
    "        plt.plot(y[0], \"k-\")\n",
    "        for idx, speaker in enumerate(set(speakers)):\n",
    "            plt.fill_between(range(len(c)), -1, 1, where=(c==speaker), alpha=0.5, label=f\"speaker_{speaker}\")\n",
    "        plt.legend(loc=\"upper center\", ncol=idx+1, bbox_to_anchor=(0.5, -0.25))\n",
    "        \n",
    "        \n",
    "class PyTorchPredictor(BasePredictor):\n",
    "    def __init__(self, config_path, model_path, max_frame=45, hop=3):\n",
    "        super().__init__(config_path, max_frame, hop)\n",
    "        \n",
    "        weight = torch.load(model_path, map_location=\"cpu\")\n",
    "        self.model = Encoder(self.ndim).to(self.device)\n",
    "        self.model.load_state_dict(weight)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def predict(self, path, plot=False):        \n",
    "        y = self._load(path, mfcc=False)\n",
    "        activity = zcr_vad(y)\n",
    "        spans = get_timestamp(activity)\n",
    "        \n",
    "        embed = [self._encode_segment(y, span) for span in spans]\n",
    "        embed = torch.cat(embed).cpu().numpy()\n",
    "        speakers = OptimizedAgglomerativeClustering().fit_predict(embed)\n",
    "        \n",
    "        if plot:\n",
    "            self._plot_diarization(y, spans, speakers)\n",
    "            \n",
    "        timestamp = np.array(spans) / self.sr\n",
    "        return timestamp, speakers\n",
    "    \n",
    "    def _encode_segment(self, y, span):\n",
    "        start, end = span\n",
    "        mfcc = self._mfcc(y[:, start:end]).to(self.device)\n",
    "        mfcc = mfcc.unfold(2, self.max_frame, self.hop).permute(2, 0, 1, 3)\n",
    "        with torch.no_grad():\n",
    "            embed = self.model(mfcc).mean(0, keepdims=True)\n",
    "        return embed\n",
    "\n",
    "    \n",
    "p = PyTorchPredictor(\"weights_best.pth\", \"configs.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fabb55c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c42a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
